{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcb5de5-47de-49d6-a15f-9352d5b2e5b2",
   "metadata": {},
   "source": [
    "# Use MACS3 API to call peaks for each cluster in scATAC-seq data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165ba1d-575a-4b1b-bda9-e6368d74ebb1",
   "metadata": {},
   "source": [
    "The scATAC-seq data was downloaded from [10X genomics website](https://www.10xgenomics.com/datasets/10k-human-pbmcs-atac-v2-chromium-x-2-standard). The files used in this tutorial are:\n",
    "- [Clustering analysis](https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_X/10k_pbmc_ATACv2_nextgem_Chromium_X_analysis.tar.gz) for the clustering results\n",
    "- [Fragments](https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_X/10k_pbmc_ATACv2_nextgem_Chromium_X_fragments.tsv.gz) for the locations of aligned fragments in TSV format\n",
    "- [Per Barcode Metrics](https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_X/10k_pbmc_ATACv2_nextgem_Chromium_X_singlecell.csv) for the information of each barcode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7f903-cc55-4f5e-aa4e-eee1b4d246ca",
   "metadata": {},
   "source": [
    "## Load the fragment file and build pileup track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ffdb6-a8e7-4174-9b7c-516a283044b8",
   "metadata": {},
   "source": [
    "We will import the fragment file parser from MACS3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8b01f2-46af-44d3-809f-840054fd166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install macs3 panda\n",
    "\n",
    "from MACS3.IO.Parser import FragParser\n",
    "from MACS3.IO.BedGraphIO import bedGraphIO\n",
    "from MACS3.Utilities.Logger import logging\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "np.seterr(all='raise')\n",
    "warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "\n",
    "# we can use the pre-configured logging function from MACS3 to monitor memory usage...\n",
    "logger = logging.getLogger(\"demo\")\n",
    "info = logger.info\n",
    "# Note, you can replace `print` with `info` to show the time and memory usage at that moment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846493c-6d7c-4a6c-9f2f-34590795c94c",
   "metadata": {},
   "source": [
    "The fragment file is `10k_pbmc_ATACv2_nextgem_Chromium_X_fragments.tsv.gz`. Note that MACS3 can directly load gzipped files. We will load the file and build a `PairEndTrack.PETrackII` object, which contains alignment locations, barcodes and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d98fd56-5884-494b-8da6-9c1d47d998dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  @ 15 Nov 2025 10:52:09: [196 MB]  1000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:09: [237 MB]  2000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:10: [257 MB]  3000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:11: [281 MB]  4000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:11: [285 MB]  5000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:12: [299 MB]  6000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:12: [310 MB]  7000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:13: [310 MB]  8000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:14: [310 MB]  9000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:14: [310 MB]  10000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:15: [328 MB]  11000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:16: [344 MB]  12000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:16: [359 MB]  13000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:17: [373 MB]  14000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:18: [399 MB]  15000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:18: [413 MB]  16000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:19: [428 MB]  17000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:20: [443 MB]  18000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:20: [458 MB]  19000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:21: [473 MB]  20000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:22: [488 MB]  21000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:22: [504 MB]  22000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:23: [517 MB]  23000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:24: [527 MB]  24000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:24: [538 MB]  25000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:25: [554 MB]  26000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:25: [565 MB]  27000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:26: [580 MB]  28000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:27: [595 MB]  29000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:27: [609 MB]  30000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:28: [623 MB]  31000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:29: [637 MB]  32000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:29: [648 MB]  33000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:30: [655 MB]  34000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:31: [660 MB]  35000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:31: [674 MB]  36000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:32: [674 MB]  37000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:33: [674 MB]  38000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:33: [678 MB]  39000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:34: [691 MB]  40000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:35: [705 MB]  41000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:35: [718 MB]  42000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:36: [731 MB]  43000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:37: [731 MB]  44000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:37: [731 MB]  45000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:38: [731 MB]  46000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:39: [731 MB]  47000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:39: [731 MB]  48000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:40: [731 MB]  49000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:41: [731 MB]  50000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:41: [731 MB]  51000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:42: [745 MB]  52000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:43: [758 MB]  53000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:43: [772 MB]  54000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:44: [786 MB]  55000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:45: [799 MB]  56000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:45: [803 MB]  57000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:46: [811 MB]  58000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:47: [820 MB]  59000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:47: [833 MB]  60000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:48: [839 MB]  61000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:49: [844 MB]  62000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:49: [851 MB]  63000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:50: [851 MB]  64000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:51: [871 MB]  65000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:51: [879 MB]  66000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:52: [893 MB]  67000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:53: [899 MB]  68000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:53: [903 MB]  69000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:54: [907 MB]  70000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:55: [915 MB]  71000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:55: [915 MB]  72000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:56: [916 MB]  73000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:57: [918 MB]  74000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:57: [918 MB]  75000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:58: [918 MB]  76000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:59: [918 MB]  77000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:52:59: [918 MB]  78000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:00: [918 MB]  79000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:01: [918 MB]  80000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:01: [918 MB]  81000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:02: [918 MB]  82000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:03: [918 MB]  83000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:03: [918 MB]  84000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:04: [918 MB]  85000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:05: [920 MB]  86000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:05: [928 MB]  87000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:06: [939 MB]  88000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:07: [950 MB]  89000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:07: [956 MB]  90000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:08: [956 MB]  91000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:09: [957 MB]  92000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:09: [971 MB]  93000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:10: [984 MB]  94000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:11: [998 MB]  95000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:11: [1011 MB]  96000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:12: [1011 MB]  97000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:13: [1017 MB]  98000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:13: [1018 MB]  99000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:14: [1026 MB]  100000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:15: [1026 MB]  101000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:15: [1035 MB]  102000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:16: [1036 MB]  103000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:17: [1044 MB]  104000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:17: [1052 MB]  105000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:18: [1057 MB]  106000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:19: [1066 MB]  107000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:19: [1071 MB]  108000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:20: [1084 MB]  109000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:21: [1084 MB]  110000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:21: [1084 MB]  111000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:22: [1084 MB]  112000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:23: [1084 MB]  113000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:23: [1084 MB]  114000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:24: [1084 MB]  115000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:25: [1084 MB]  116000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:25: [1084 MB]  117000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:26: [1084 MB]  118000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:27: [1084 MB]  119000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:27: [1084 MB]  120000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:28: [1084 MB]  121000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:29: [1084 MB]  122000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:29: [1084 MB]  123000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:30: [1084 MB]  124000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:31: [1084 MB]  125000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:31: [1084 MB]  126000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:32: [1086 MB]  127000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:33: [1099 MB]  128000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:33: [1113 MB]  129000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:34: [1126 MB]  130000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:35: [1139 MB]  131000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:35: [1153 MB]  132000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:36: [1161 MB]  133000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:37: [1167 MB]  134000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:37: [1173 MB]  135000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:38: [1184 MB]  136000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:39: [1185 MB]  137000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:39: [1189 MB]  138000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:40: [1189 MB]  139000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:41: [1193 MB]  140000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:41: [1199 MB]  141000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:42: [1204 MB]  142000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:43: [1207 MB]  143000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:44: [1207 MB]  144000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:44: [1215 MB]  145000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:45: [1228 MB]  146000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:46: [1231 MB]  147000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:46: [1235 MB]  148000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:47: [1246 MB]  149000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:47: [1255 MB]  150000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:48: [1261 MB]  151000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:49: [1261 MB]  152000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:49: [1268 MB]  153000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:50: [1281 MB]  154000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:51: [1295 MB]  155000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:52: [1308 MB]  156000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:52: [1321 MB]  157000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:53: [1335 MB]  158000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:54: [1348 MB]  159000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:54: [1360 MB]  160000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:55: [1363 MB]  161000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:56: [1364 MB]  162000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:56: [1376 MB]  163000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:57: [1386 MB]  164000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:58: [1395 MB]  165000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:58: [1408 MB]  166000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:53:59: [1422 MB]  167000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:00: [1435 MB]  168000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:00: [1445 MB]  169000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:01: [1451 MB]  170000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:02: [1454 MB]  171000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:02: [1469 MB]  172000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:03: [1479 MB]  173000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:04: [1480 MB]  174000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:04: [1480 MB]  175000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:05: [1487 MB]  176000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:06: [1501 MB]  177000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:07: [1514 MB]  178000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:07: [1527 MB]  179000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:08: [1537 MB]  180000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:09: [1543 MB]  181000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:09: [1546 MB]  182000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:10: [1560 MB]  183000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:11: [1575 MB]  184000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:11: [1584 MB]  185000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:12: [1597 MB]  186000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:13: [1611 MB]  187000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:13: [1624 MB]  188000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:14: [1637 MB]  189000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:15: [1651 MB]  190000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:16: [1664 MB]  191000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:16: [1677 MB]  192000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:17: [1689 MB]  193000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:18: [1692 MB]  194000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:18: [1697 MB]  195000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:19: [1707 MB]  196000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:20: [1707 MB]  197000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:20: [1707 MB]  198000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:21: [1707 MB]  199000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:22: [1707 MB]  200000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:22: [1707 MB]  201000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:23: [1707 MB]  202000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:24: [1707 MB]  203000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:25: [1710 MB]  204000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:25: [1710 MB]  205000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:26: [1711 MB]  206000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:27: [1723 MB]  207000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:27: [1731 MB]  208000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:28: [1736 MB]  209000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:29: [1736 MB]  210000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:29: [1745 MB]  211000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:30: [1758 MB]  212000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:31: [1768 MB]  213000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:32: [1772 MB]  214000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:32: [1773 MB]  215000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:33: [1779 MB]  216000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:34: [1779 MB]  217000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:34: [1779 MB]  218000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:35: [1782 MB]  219000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:36: [1792 MB]  220000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:36: [1792 MB]  221000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:37: [1803 MB]  222000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:38: [1804 MB]  223000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:39: [1810 MB]  224000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:39: [1819 MB]  225000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:40: [1821 MB]  226000000 fragments parsed \n",
      "INFO  @ 15 Nov 2025 10:54:41: [1825 MB]  227000000 fragments parsed \n"
     ]
    }
   ],
   "source": [
    "frag_file = FragParser(\"./10k_pbmc_ATACv2_nextgem_Chromium_X_fragments.tsv.gz\", buffer_size=100000)\n",
    "petrack = frag_file.build_petrack(max_count=2)\n",
    "petrack.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1fa2ff-913b-452f-93f6-7366c5485a5b",
   "metadata": {},
   "source": [
    "Before we call peaks, let's clean up the data by removing uncommon chromosomes from downstream analysis. Let's check first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b866cc28-bc1e-4b5a-b280-b240e7fd3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'GL000009.2', b'GL000194.1', b'GL000195.1', b'GL000205.2', b'GL000213.1', b'GL000218.1', b'GL000219.1', b'KI270711.1', b'KI270713.1', b'KI270721.1', b'KI270726.1', b'KI270727.1', b'KI270728.1', b'KI270731.1', b'KI270734.1', b'chr1', b'chr10', b'chr11', b'chr12', b'chr13', b'chr14', b'chr15', b'chr16', b'chr17', b'chr18', b'chr19', b'chr2', b'chr20', b'chr21', b'chr22', b'chr3', b'chr4', b'chr5', b'chr6', b'chr7', b'chr8', b'chr9', b'chrX', b'chrY']\n"
     ]
    }
   ],
   "source": [
    "petrack_rlengths = petrack.rlengths\n",
    "print(list(petrack_rlengths.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e4b49-cee6-443b-8622-83024394a2e6",
   "metadata": {},
   "source": [
    "Let's only keep chr1 to chrY. In order to do so, we need to construct genomic 'regions' covering the uncommon chromosomes, and then 'exclude' any fragments overlapping with those regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6bd3132-4cad-4131-bd88-b94230cd895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MACS3.Signal.Region import Regions\n",
    "\n",
    "regions_to_be_excluded = Regions()\n",
    "for chrom, chrom_length in petrack_rlengths.items():\n",
    "    if not chrom.startswith(b'chr'): # this rule may not work if you want to exclude chomosome such as chr1_random, so adjust it if it's necessary\n",
    "        regions_to_be_excluded.add_loc(chrom, 0, chrom_length)\n",
    "\n",
    "# then use .exclude of PETrackII \n",
    "petrack.exclude(regions_to_be_excluded)\n",
    "petrack.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec2653-92e4-4c80-b513-72448f7d8911",
   "metadata": {},
   "source": [
    "Now check the chromosome names again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3c8f90-fc19-4a69-b5b1-63c9073652b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'chr1', b'chr10', b'chr11', b'chr12', b'chr13', b'chr14', b'chr15', b'chr16', b'chr17', b'chr18', b'chr19', b'chr2', b'chr20', b'chr21', b'chr22', b'chr3', b'chr4', b'chr5', b'chr6', b'chr7', b'chr8', b'chr9', b'chrX', b'chrY']\n"
     ]
    }
   ],
   "source": [
    "petrack_rlengths = petrack.rlengths\n",
    "print(list(petrack_rlengths.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d52e7-635e-4d6d-82c7-eabb51267865",
   "metadata": {},
   "source": [
    "Now we can check some basic statistics of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6427cb03-4e17-4244-8c6d-541432e69b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average template length is 153.16387939453125\n",
      "total number of fragments is 331064421\n"
     ]
    }
   ],
   "source": [
    "print(f\"average template length is {petrack.average_template_length}\")\n",
    "print(f\"total number of fragments is {petrack.total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905710b-8c72-48ce-b089-692658779bd1",
   "metadata": {},
   "source": [
    "## Call peaks for the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769784d-db9e-46a8-a0ec-b73ecac18191",
   "metadata": {},
   "source": [
    "Next, we will call peaks for the entire dataset. The first step is to build a pileup track from the alignments. But before that, we can filter out invalid cells/barcodes. For example, we have an `10k_pbmc_ATACv2_nextgem_Chromium_X_singlecell.csv` file downloaded together with the fragment file which is from the standard 10x pipeline. If we want to only keep the cell barcodes with at least 500 usable fragments and at least 25% of total fragments are marked as usable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd45d34e-6c6c-4ef1-a403-d86438c8f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell barcodes to be kept: 10270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "barcodes = []\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(\"./10k_pbmc_ATACv2_nextgem_Chromium_X_singlecell.csv\")\n",
    "\n",
    "# We keep the cell barcode that 1) is a cell barcode 2) more than 500 usable fragments 3) at least 25% of total fragments are usable\n",
    "df_pass = df[\n",
    "    (df[\"is__cell_barcode\"] == 1) & \n",
    "    (df[\"passed_filters\"] >= 500) &\n",
    "    (df[\"passed_filters\"]/df[\"total\"] >= 0.25)]\n",
    "\n",
    "# Extract barcode values as a list then convert to a set\n",
    "# Note: we need to encode the string since PETrackII.subset needs bytestrings.\n",
    "barcodes = set([x.encode() for x in df_pass[\"barcode\"].tolist()])\n",
    "\n",
    "print(\"Cell barcodes to be kept:\", len(barcodes))\n",
    "\n",
    "petrack = petrack.subset(barcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d801f-5500-450d-8c18-69847a03df9a",
   "metadata": {},
   "source": [
    "Now we can build the pileup signal track of all valid cell barcodes and their fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08be5b9-6dd5-4718-a4ff-390e32c43dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pileup_track = petrack.pileup_bdg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e429e-bde9-4e3b-bb11-df34480390f3",
   "metadata": {},
   "source": [
    "We can get the sum, total length, maximum, minimum, mean, and standard deviation from the pileup track by using `summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff4789b-9cc9-4891-8e0d-f4256dba26ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pileup_sum=29146726400.0\n",
      "pileup_length=3086966835\n",
      "pileup_max=7802.0\n",
      "pileup_min=0.0\n",
      "pileup_mean=9.441865921020508\n",
      "pileup_std=97.03312683105469\n"
     ]
    }
   ],
   "source": [
    "(pileup_sum, pileup_length, pileup_max, pileup_min, pileup_mean, pileup_std) = pileup_track.summary()\n",
    "print(f\"{pileup_sum=}\\n{pileup_length=}\\n{pileup_max=}\\n{pileup_min=}\\n{pileup_mean=}\\n{pileup_std=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81610f-be87-4c50-87f4-aaedb0d2eb21",
   "metadata": {},
   "source": [
    "Next, we will call peaks for the entire dataset. Since we are calling peaks for ATAC-seq data, we will use the single whole-genome average pileup value as the background. From above calculation, the average is in the variable `pileup_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40cf3ac-e926-416e-81a5-f87fc826559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_bg_track = pileup_track.set_single_value(pileup_mean) # this will return a new track with all values set as `pileup_mean`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7f997-e29e-44bc-8e47-cbffbe8de978",
   "metadata": {},
   "source": [
    "We construct a score track for comparing observed pileup and background. The score track will contain the observed pileup, the background, and scores for each position in the genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f753cd4-a1db-464f-8f4f-46e86c40f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_track = pileup_track.make_ScoreTrackII_for_macs(global_bg_track, depth1=100, depth2=100) # Note, depth1 and 2 are the same so there is no need for scaling the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bdee57-a140-4f0b-9ead-9305e87476f9",
   "metadata": {},
   "source": [
    "We will use p-score (-log10 p-value) as scores. MACS3 supports the following methods:\n",
    "\n",
    "- p: -log10 pvalue;\n",
    "- q: -log10 qvalue;\n",
    "- l: log10 likelihood ratio (minus for depletion)\n",
    "- s: symmetric log10 likelihood ratio (for comparing two ChIPs)\n",
    "- f: log10 fold enrichment\n",
    "- F: linear fold enrichment\n",
    "- d: subtraction\n",
    "- M: maximum\n",
    "- m: fragment pileup per million reads\n",
    "\n",
    "To use any of the methods, provide the argument to `change_score_method` with `ord`, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b85280-589a-43bf-a6ba-5a925a0145ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_track.change_score_method(ord('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea47967-0c05-4d3c-954e-84160e66dc96",
   "metadata": {},
   "source": [
    "Then we can call peaks based on these newly generated score track. We will use `p-value`=0.00001 as cutoff, which will be translated into p-score cutoff of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028f0738-882d-49df-b86e-6695a926fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = score_track.call_peaks(cutoff=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488ebbf-12f3-45b1-a9e6-cc9906060918",
   "metadata": {},
   "source": [
    "We can check the number of peaks and the total length of peaks by converting peaks to regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069c942b-dfec-48bb-a5ed-b234b1493aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks: 254367\n",
      "Total basepairs of peaks: 234727859\n"
     ]
    }
   ],
   "source": [
    "# make Regions for peaks from all cell barcodes\n",
    "regions = Regions()\n",
    "regions.init_from_PeakIO(peaks)\n",
    "print(f\"Number of peaks: {regions.total}\")\n",
    "print(f\"Total basepairs of peaks: {regions.total_length()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "223681f8-e67b-4a92-8870-3814037cacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"peaks.narrowPeak\",\"w\") as f:\n",
    "    peaks.write_to_narrowPeak(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59c6c1-8841-4bd0-8962-e782af1eda87",
   "metadata": {},
   "source": [
    "## Call peaks for the certain cluster of cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91572cf4-ac9f-468a-8312-33e8c235da04",
   "metadata": {},
   "source": [
    "In this section, we will illustrate how to call peaks for a subset of cells from clustering analysis. Assuming that we have clustering results saved in the file 'analysis/clustering/graphclust/clusters.csv'. We downloaded the results from 10X website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fdb5b9b-3477-4ff9-95cc-85941b3c2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cell barcodes in cluster 9: 620\n",
      "Number of cell barcodes in cluster 3: 813\n",
      "Number of cell barcodes in cluster 1: 1222\n",
      "Number of cell barcodes in cluster 7: 639\n",
      "Number of cell barcodes in cluster 6: 658\n",
      "Number of cell barcodes in cluster 4: 772\n",
      "Number of cell barcodes in cluster 2: 997\n",
      "Number of cell barcodes in cluster 5: 770\n",
      "Number of cell barcodes in cluster 8: 621\n",
      "Number of cell barcodes in cluster 12: 428\n",
      "Number of cell barcodes in cluster 13: 379\n",
      "Number of cell barcodes in cluster 11: 477\n",
      "Number of cell barcodes in cluster 10: 537\n",
      "Number of cell barcodes in cluster 14: 269\n",
      "Number of cell barcodes in cluster 18: 226\n",
      "Number of cell barcodes in cluster 15: 262\n",
      "Number of cell barcodes in cluster 19: 102\n",
      "Number of cell barcodes in cluster 16: 246\n",
      "Number of cell barcodes in cluster 17: 235\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV from clustering analysis\n",
    "df = pd.read_csv(\"./analysis/clustering/graphclust/clusters.csv\")\n",
    "\n",
    "# Get barcodes for each cluster\n",
    "barcodes_cluster_dict = {}\n",
    "for c in df[\"Cluster\"].unique():\n",
    "    df_c = df[df[\"Cluster\"] == c]\n",
    "    barcodes_cluster_dict[c] = set([x.encode() for x in df_c[\"Barcode\"].tolist()])\n",
    "    print(f\"Number of cell barcodes in cluster {c}: {len(barcodes_cluster_dict[c])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d7b3a-074d-4826-a760-04f86e69fe13",
   "metadata": {},
   "source": [
    "In the following analysis, we will focus on cluster 1 and cluster 10 -- the biggest cluster and the smallest cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54c52b9-1aef-4c22-af09-7bfe8a93fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subset of fragments for c1\n",
    "petrack_c1 = petrack.subset(barcodes_cluster_dict[1])\n",
    "\n",
    "# Take subset of fragments for c10\n",
    "petrack_c10 = petrack.subset(barcodes_cluster_dict[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a242249-25f5-4c44-8cce-813c32b6a559",
   "metadata": {},
   "source": [
    "Let's get some basic stats of the two subsets of fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2383bd-ed97-495f-a320-9e5e503d6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fragments for cluster 1: 35551266\n",
      "Number of fragments for cluster 10: 16631656\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of fragments for cluster 1: {petrack_c1.total}\")\n",
    "print(f\"Number of fragments for cluster 10: {petrack_c10.total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60605e42-1377-4b3e-862a-7bea2c448705",
   "metadata": {},
   "source": [
    "Similar to previous steps, we can generate pileup tracks for these two clusters, make the background tracks, calculate scores, and then call peaks with certain cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4ede95c-3e8c-4c27-a9ad-61c216d005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cluster 1\n",
    "pileup_track_c1 = petrack_c1.pileup_bdg()\n",
    "global_bg_track_c1 = pileup_track_c1.set_single_value(pileup_track_c1.summary()[4]) \n",
    "score_track_c1 = pileup_track_c1.make_ScoreTrackII_for_macs(global_bg_track_c1, depth1=100, depth2=100)\n",
    "score_track_c1.change_score_method(ord('p'))\n",
    "peaks_c1 = score_track_c1.call_peaks(cutoff=5)\n",
    "\n",
    "# For cluster 10\n",
    "pileup_track_c10 = petrack_c10.pileup_bdg()\n",
    "global_bg_track_c10 = pileup_track_c10.set_single_value(pileup_track_c10.summary()[4]) \n",
    "score_track_c10 = pileup_track_c10.make_ScoreTrackII_for_macs(global_bg_track_c10, depth1=100, depth2=100)\n",
    "score_track_c10.change_score_method(ord('p'))\n",
    "peaks_c10 = score_track_c10.call_peaks(cutoff=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6615699-820d-405c-a787-ceb6e5659bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pileup_sum=5064676352.0\n",
      "pileup_length=3086780615\n",
      "pileup_max=940.0\n",
      "pileup_min=0.0\n",
      "pileup_mean=1.640763282775879\n",
      "pileup_std=12.390028953552246\n"
     ]
    }
   ],
   "source": [
    "(pileup_sum, pileup_length, pileup_max, pileup_min, pileup_mean, pileup_std) = pileup_track_c1.summary()\n",
    "print(f\"{pileup_sum=}\\n{pileup_length=}\\n{pileup_max=}\\n{pileup_min=}\\n{pileup_mean=}\\n{pileup_std=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f18217-516e-4103-9867-609eafc9bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pileup_sum=2456030464.0\n",
      "pileup_length=3086611397\n",
      "pileup_max=414.0\n",
      "pileup_min=0.0\n",
      "pileup_mean=0.7957044243812561\n",
      "pileup_std=5.5370354652404785\n"
     ]
    }
   ],
   "source": [
    "(pileup_sum, pileup_length, pileup_max, pileup_min, pileup_mean, pileup_std) = pileup_track_c10.summary()\n",
    "print(f\"{pileup_sum=}\\n{pileup_length=}\\n{pileup_max=}\\n{pileup_min=}\\n{pileup_mean=}\\n{pileup_std=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c663c85-7aa2-47fe-a52d-e8453266ee9e",
   "metadata": {},
   "source": [
    "Let's check some basic facts of the peaks called from only the cluster 1 or cluster 10 cell barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbe99af-ce05-4806-bd23-800a003e77bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks of cluster 1: 66369\n",
      "Total basepairs of peaks of cluster 1: 57119357\n",
      "Number of peaks of cluster 10: 54800\n",
      "Total basepairs of peaks of cluster 10: 39590493\n"
     ]
    }
   ],
   "source": [
    "regions_c1 = Regions()\n",
    "regions_c1.init_from_PeakIO(peaks_c1)\n",
    "print(f\"Number of peaks of cluster 1: {regions_c1.total}\")\n",
    "print(f\"Total basepairs of peaks of cluster 1: {regions_c1.total_length()}\")\n",
    "\n",
    "regions_c10 = Regions()\n",
    "regions_c10.init_from_PeakIO(peaks_c10)\n",
    "print(f\"Number of peaks of cluster 10: {regions_c10.total}\")\n",
    "print(f\"Total basepairs of peaks of cluster 10: {regions_c10.total_length()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fddd033-af95-4923-be32-46fcfeec05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"peaks_c1.narrowPeak\",\"w\") as f:\n",
    "    peaks_c1.write_to_narrowPeak(f)\n",
    "\n",
    "with open(\"peaks_c10.narrowPeak\",\"w\") as f:\n",
    "    peaks_c10.write_to_narrowPeak(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581f94e-6a9c-4afd-adbb-56afec4f911d",
   "metadata": {},
   "source": [
    "## Compare cluster-specific peaks\n",
    "\n",
    "We can try to compare these two sets of peaks using functions provided by `Regions` class. Such as get the overlapping regions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687cd051-a6e0-4780-baa3-1ea6e5682de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping regions: 37992\n",
      "And total basepairs of overlapping regions: 28824472\n"
     ]
    }
   ],
   "source": [
    "overlapped_regions_c1_c10 = regions_c1.intersect(regions_c10)\n",
    "\n",
    "print(f\"Number of overlapping regions: {overlapped_regions_c1_c10.total}\")\n",
    "print(f\"And total basepairs of overlapping regions: {overlapped_regions_c1_c10.total_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9961b4-e2a8-4c3f-a6a4-30c31c0bb3b3",
   "metadata": {},
   "source": [
    "Or unique regions/peaks of cluster 10 that can not be found in the peaks called from cluster 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d807bef6-6d25-43d9-8f9d-1ccc30b1028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Regions for c10 peaks again since later we will alter this object with .exclude function\n",
    "unique_regions_c10_vs_c1 = Regions()\n",
    "unique_regions_c10_vs_c1.init_from_PeakIO(peaks_c10)\n",
    "# then\n",
    "unique_regions_c10_vs_c1.exclude(regions_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18ea698c-8d57-44d5-87f3-9f96b29d69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peaks of cluster 10 - cluster 1: 17593\n",
      "And total basepairs in peaks of cluster 10 - cluster 1: 8625919\n"
     ]
    }
   ],
   "source": [
    "print(f\"Peaks of cluster 10 - cluster 1: {unique_regions_c10_vs_c1.total}\")\n",
    "print(f\"And total basepairs in peaks of cluster 10 - cluster 1: {unique_regions_c10_vs_c1.total_length()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a77025b-955e-4b72-9a44-b8635381a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"peaks_c10_vs_c1.bed\",\"w\") as f:\n",
    "    unique_regions_c10_vs_c1.write_to_bed(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5a818-e2e2-43a0-87ba-1060e4614b19",
   "metadata": {},
   "source": [
    "## Missed rare cluster-specific peaks\n",
    "\n",
    "Some rare cluster-specific peaks may be missing while we call peaks using all cell barcodes. Therefore, we can compare the cluster-specific peaks with the peaks called from all barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba34d253-d800-4bca-8c6b-ef8ddae5b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Regions for c10 peaks again since later we will alter this object with .exclude function\n",
    "unique_regions_c10 = Regions()\n",
    "unique_regions_c10.init_from_PeakIO(peaks_c10)\n",
    "# then\n",
    "unique_regions_c10.exclude(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a6f97-5ae3-4094-a823-b608e76c4be9",
   "metadata": {},
   "source": [
    "Yes. We do identify rare peaks for cluster 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26dedff3-7f1c-47f8-9c2d-aedea92b640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare peaks in cluster 10: 1400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rare peaks in cluster 10: {unique_regions_c10.total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe5f94cb-be2e-4987-8d70-ba02d5bab803",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_peaks_c10.bed\",\"w\") as f:\n",
    "    unique_regions_c10.write_to_bed(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
